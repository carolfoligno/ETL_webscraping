{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300f77da",
   "metadata": {},
   "source": [
    "# ETL - H&M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97aa896",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddae6a0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# importando as bibliotecas\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead5fdd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data collection - VITRINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bac166",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>scrapy_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256001</td>\n",
       "      <td>men_jeans_slim</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>$ 19.99</td>\n",
       "      <td>2022-09-16 15:56:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>men_jeans_slim</td>\n",
       "      <td>Slim Jeans</td>\n",
       "      <td>$ 19.99</td>\n",
       "      <td>2022-09-16 15:56:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0993887007</td>\n",
       "      <td>men_jeans_joggers</td>\n",
       "      <td>Hybrid Regular Denim Joggers</td>\n",
       "      <td>$ 39.99</td>\n",
       "      <td>2022-09-16 15:56:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0938875007</td>\n",
       "      <td>men_jeans_slim</td>\n",
       "      <td>Slim Tapered Jeans</td>\n",
       "      <td>$ 39.99</td>\n",
       "      <td>2022-09-16 15:56:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004199007</td>\n",
       "      <td>men_jeans_skinny</td>\n",
       "      <td>Skinny Cropped Jeans</td>\n",
       "      <td>$ 29.99</td>\n",
       "      <td>2022-09-16 15:56:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id   product_category                  product_name    price  \\\n",
       "0  1024256001     men_jeans_slim                    Slim Jeans  $ 19.99   \n",
       "1  1024256006     men_jeans_slim                    Slim Jeans  $ 19.99   \n",
       "2  0993887007  men_jeans_joggers  Hybrid Regular Denim Joggers  $ 39.99   \n",
       "3  0938875007     men_jeans_slim            Slim Tapered Jeans  $ 39.99   \n",
       "4  1004199007   men_jeans_skinny          Skinny Cropped Jeans  $ 29.99   \n",
       "\n",
       "       scrapy_datetime  \n",
       "0  2022-09-16 15:56:13  \n",
       "1  2022-09-16 15:56:13  \n",
       "2  2022-09-16 15:56:13  \n",
       "3  2022-09-16 15:56:13  \n",
       "4  2022-09-16 15:56:13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paginação em HTML\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "page = requests.get( url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup( page.text, 'html.parser')\n",
    "\n",
    "# pegando o total items que tem a vitrine\n",
    "total_item = soup.find_all('h2', class_='load-more-heading')[0].get('data-total')\n",
    "\n",
    "# descobrindo o numero de paginas\n",
    "# o comando np.ceil arrendonda para o numero interio acima\n",
    "page_number = np.ceil( int(total_item) / 36 )\n",
    "\n",
    "# montando o url do total de produtos da vitrine\n",
    "url2 = url + '?page-size=' + str( int(page_number*36))\n",
    "\n",
    "\n",
    "# ======================== COLETAR DADOS NA VITRINE ====================================\n",
    "# requests irá pegar as informações da pagina html é salvar numa variavel\n",
    "#url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "# o headers é para burlar o html fingindo que é um browser e não python que está captando as informações\n",
    "# link dos headers de diversos browser : https://www.useragentstring.com/pages/useragentstring.php\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "page = requests.get( url2, headers=headers)\n",
    "\n",
    "# toda a informação da pagina html está na variavel soup\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# salvar em uma variavel apenas as informações da vitrine \n",
    "# observando a pagina HTML no site (inspecionar)\n",
    "\n",
    "# comando .find - pega a primeira informação\n",
    "products = soup.find('ul', class_='products-listing small')\n",
    "\n",
    "# o comando find_all retorna as informações como lista\n",
    "product_list = products.find_all('article', class_='hm-product-item')\n",
    "\n",
    "# colunas da simulação que necessito ter para meu projeto\n",
    "# id | product_category | product_name | price | color | composition\n",
    "\n",
    "# coluna id \n",
    "product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "# coluna product_category\n",
    "product_category = [p.get('data-category') for p in product_list]\n",
    "\n",
    "# coluna product_name\n",
    "# para pegar as informações do name\n",
    "# seleciono outra tag dentro d products\n",
    "product_list = products.find_all('a', class_='link')\n",
    "\n",
    "# para pegar apenas os nomes utilizo o comando get_text\n",
    "product_name = [p.get_text('') for p in product_list]\n",
    "\n",
    "# para pegar os preço de cada calça devo selecionar outra tag\n",
    "product_list = products.find_all('span', class_='price regular')\n",
    "price = [p.get_text() for p in product_list]\n",
    "\n",
    "# juntar as colunas em um dataframe utilizando Pandas\n",
    "data = pd.DataFrame( [product_id, product_category, product_name, price]).T\n",
    "data.columns = ['product_id', 'product_category', 'product_name','price']\n",
    "\n",
    "# cada scrapy é importante colocar datar\n",
    "data['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e70a90",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data collection by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028597a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# empty dataframe\n",
    "df_compositions = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # API requests\n",
    "\n",
    "    # url do unico produto\n",
    "    url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i, 'product_id'] + '.html'\n",
    "#     url = 'https://www2.hm.com/en_us/productpage.' + '1024256001' + '.html'\n",
    "\n",
    "    page = requests.get( url, headers=headers)\n",
    "\n",
    "    #BeautifulSoup object\n",
    "    soup = BeautifulSoup( page.text, 'html.parser')\n",
    "\n",
    "    # =================================COLOR NAME ===============================================================\n",
    "\n",
    "    # seleciona a tag a com a class filter.. para pegar(get) a class que tem a informa desejada -- que é a cor\n",
    "    product_list = soup.find_all('a', class_='filter-option miniature active') + soup.find_all('a', class_='filter-option miniature')\n",
    "    color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "    # pegando cada id da cor do produto\n",
    "    product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "\n",
    "    # montando um dataframe\n",
    "    df_color = pd.DataFrame([product_id, color_name]).T\n",
    "    df_color.columns = ['product_id', 'color_name']\n",
    "    \n",
    "    for j in range(len(df_color)):\n",
    "        # API requests\n",
    "\n",
    "        # url do unico produto\n",
    "        url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j, 'product_id'] + '.html'\n",
    "\n",
    "        page = requests.get( url, headers=headers)\n",
    "\n",
    "        #BeautifulSoup object\n",
    "        soup = BeautifulSoup( page.text, 'html.parser')\n",
    "        \n",
    "        #======================================== Product Name ============================\n",
    "        product_name = soup.find_all('hm-product-name')\n",
    "        product_name = product_name[0].get_text().replace('\\n', '')\n",
    "        \n",
    "        #======================================== Product Price ============================\n",
    "        product_price = soup.find_all('div',class_ = 'primary-row product-item-price')\n",
    "        product_price = re.findall(r'\\d+\\.\\d+', product_price[0].get_text() )[0]\n",
    "        product_price\n",
    "        \n",
    "        # ================================= COMPOSITION ===============================================================\n",
    "        # pegando as informações dos detalhes da calça\n",
    "        product_composition_list = soup.find_all('div',class_=\"details-attributes-list-item\")\n",
    "        product_composition = [list(filter(None, p.get_text().split('\\n'))) for p in product_composition_list]\n",
    "\n",
    "        # fazendo o dataframe da lista de detalhes da calça\n",
    "        df_aux = pd.DataFrame(product_composition).T\n",
    "        df_aux.columns = df_aux.iloc[0]\n",
    "\n",
    "        # deletar a primeira linha\n",
    "        df_aux = df_aux[1:]\n",
    "\n",
    "        # selecionar apenas as colunas desejadas\n",
    "        df_composition = df_aux[['Fit', 'Composition', 'Art. No.']]\n",
    "        #excluindo apenas as linhas onde todos os valores estão vazios\n",
    "        df_composition = df_composition.dropna(how='all',axis=0)\n",
    "\n",
    "        # preenchendo os valores vazios\n",
    "        df_composition = df_composition.fillna( method='ffill')\n",
    "\n",
    "        # remover pocket lining, shell and lining\n",
    "        df_composition['Composition'] = df_composition['Composition'].replace('Pocket lining: ', '', regex=True)\n",
    "        df_composition['Composition'] = df_composition['Composition'].replace('Shell: ', '', regex=True)\n",
    "        df_composition['Composition'] = df_composition['Composition'].replace('Lining: ', '', regex=True)\n",
    "        df_composition['Composition'] = df_composition['Composition'].replace('Pocket: ', '', regex=True)\n",
    "        \n",
    "        # rename columns\n",
    "        df_composition.columns = ['fit', 'composition', 'product_id']\n",
    "        \n",
    "        # recebendo novas colunas\n",
    "        df_composition['product_name'] = product_name\n",
    "        df_composition['product_price'] = product_price\n",
    "\n",
    "        # ================================= JUNTANDO AS INFORMAÇÕES ==============================================================\n",
    "\n",
    "        df_composition = pd.merge(df_composition, df_color, how='left', on='product_id')\n",
    "\n",
    "        # concat todos os dataframes\n",
    "        df_compositions = pd.concat( [df_compositions, df_composition], axis=0 )\n",
    "\n",
    "\n",
    "# gerando style id + color id\n",
    "# df_color\n",
    "\n",
    "df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3])\n",
    "df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "# cada scrapy é importante colocar datar\n",
    "df_compositions['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937a400",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a851687c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# product id\n",
    "df = df_compositions.dropna(subset=['product_id'])\n",
    "\n",
    "df['product_name'] = df['product_name'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "df['product_price'] = df['product_price'].astype(float)\n",
    "\n",
    "df['fit'] = df['fit'].apply(lambda x: x.replace(' ', '_').lower() if pd.notnull(x) else x )\n",
    "\n",
    "df['color_name'] = df['color_name'].apply(lambda x: x.replace(' ', '_').lower() if pd.notnull(x) else x )\n",
    "\n",
    "\n",
    "# ================= composition =================================\n",
    "# quebrar a coluna composition por virgula\n",
    "# expand me retorna como dataframe\n",
    "# TENHO UM DATAFRAME COM OS NOMES SEPARADOS, MAS FORAM DE ORDEM DE COLUNA\n",
    "# O INDEX ESTÁ CORRETO\n",
    "df1 = df['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
    "\n",
    "# primeira coisa\n",
    "# definir quais as colunas que eu quero\n",
    "### df['Composition'].unique()\n",
    "# cotton | spandex | polyester |\n",
    "# criando um dataframe de referencia das colunas que eu quero\n",
    "# esse dataframe tem que ter o mesmo tamanho de linhas do dataframe que eu quero concatenar\n",
    "df_ref = pd.DataFrame( index=np.arange( len( df ) ), columns=['cotton', 'spandex', 'polyester'] )\n",
    "\n",
    "########## cotton\n",
    "# criar um dataframe de uma unica coluna, ou seja uma series\n",
    "# o df1[0] e df1[1] contem apenas os cottons\n",
    "df_cotton_0 = df1.loc[df1[0].str.contains('Cotton', na=True), 0]\n",
    "df_cotton_0.name = 'cotton'\n",
    "\n",
    "df_cotton_1 = df1.loc[df1[1].str.contains('Cotton', na= True), 1]\n",
    "df_cotton_1.name = 'cotton'\n",
    "\n",
    "# combine\n",
    "df_cotton = df_cotton_0.combine_first(df_cotton_1)\n",
    "\n",
    "# # a concateneação é atraves do index\n",
    "df_ref = pd.concat( [df_ref, df_cotton], axis=1 )\n",
    "# # ecluir colunas duplicadas (deixando a ultima [last])\n",
    "df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "# ########### spandex\n",
    "# # o df1[1] e [2] contem spandex\n",
    "df_spandex_1 = df1.loc[df1[1].str.contains('Spandex', na=True),1]\n",
    "df_spandex_1.name = 'spandex'\n",
    "\n",
    "df_spandex_2 = df1.loc[df1[2].str.contains('Spandex', na= True), 2]\n",
    "df_spandex_2.name = 'spandex'\n",
    "\n",
    "# combine\n",
    "df_spandex = df_spandex_1.combine_first(df_spandex_2)\n",
    "\n",
    "# # a concateneação é atraves do index\n",
    "df_ref = pd.concat( [df_ref, df_spandex], axis=1 )\n",
    "# # ecluir colunas duplicadas (deixando a ultima [last])\n",
    "df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "\n",
    "# #polyester\n",
    "# # o df1[0] e df1[1] contem apenas os cottons\n",
    "df_polyester_0 = df1.loc[df1[0].str.contains('Polyester', na=True),0]\n",
    "df_polyester_0.name = 'polyester'\n",
    "\n",
    "df_polyester_1 = df1.loc[df1[1].str.contains('Polyester', na= True), 1]\n",
    "df_polyester_1.name = 'polyester'\n",
    "\n",
    "df_polyester = df_polyester_0.combine_first( df_polyester_1 )\n",
    "\n",
    "# # a concateneação é atraves do index\n",
    "df_ref = pd.concat( [df_ref, df_polyester], axis=1 )\n",
    "# # ecluir colunas duplicadas (deixando a ultima [last])\n",
    "df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "# join of combine with product_id\n",
    "df_aux = pd.concat( [df['product_id'].reset_index(drop=True), df_ref], axis=1 )\n",
    "\n",
    "# pegando apenas os número das composições\n",
    "df_aux['cotton'] = df_aux['cotton'].apply( lambda x: int(re.search('\\d+',x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "df_aux['spandex'] = df_aux['spandex'].apply( lambda x: int(re.search('\\d+',x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "df_aux['polyester'] = df_aux['polyester'].apply( lambda x: int(re.search('\\d+',x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "\n",
    "# # final join\n",
    "df_aux = df_aux.groupby('product_id').max().reset_index().fillna(0)\n",
    "df = pd.merge( df, df_aux, on='product_id', how='left' )\n",
    "\n",
    "\n",
    "# drop columns \n",
    "df = df.drop( columns= ['composition'])\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f151d92",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850d9883",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mudando as ordens da coluna\n",
    "data_insert = df[[\n",
    "    'product_id',\n",
    "    'style_id',\n",
    "    'color_id',\n",
    "    'product_name',\n",
    "    'product_price',\n",
    "    'fit',\n",
    "    'color_name',\n",
    "    'cotton',\n",
    "    'spandex',\n",
    "    'polyester',\n",
    "    'scrapy_datetime'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63a5a77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # criando uma query de tabela\n",
    "# query_showroom_schema = \"\"\"\n",
    "#     CREATE TABLE vitrine(\n",
    "#     product_id,\n",
    "#     style_id,\n",
    "#     color_id,\n",
    "#     product_name,\n",
    "#     product_price,\n",
    "#     fit,\n",
    "#     color_name,\n",
    "#     cotton,\n",
    "#     spandex,\n",
    "#     polyester,\n",
    "#     scrapy_datetime\n",
    "#     )\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e2fb04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create table\n",
    "conn = sqlite3.connect('database_hm.sqlite3')\n",
    "cursor = conn.execute( query_showroom_schema)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2dbf48",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create database connection\n",
    "conn = create_engine('sqlite:///database_hm.sqlite', echo=False)\n",
    "\n",
    "#data insert\n",
    "data_insert.to_sql('vitrine', con=conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314377a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
